"""
å¢å¼ºæ•°æ®æ¨¡å— - ä½¿ç”¨å®Œæ•´çš„å†å²æ•°æ®
è§£å†³30æ¡æ•°æ®é™åˆ¶é—®é¢˜ï¼Œæ”¯æŒæœ¬åœ°æ•°æ®å’Œå®æ—¶æ•°æ®
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import glob

class EnhancedDataModule:
    """å¢å¼ºæ•°æ®æ¨¡å— - æ”¯æŒå®Œæ•´å†å²æ•°æ®"""
    
    def __init__(self):
        # ç®€åŒ–æ—¥å¿—åˆå§‹åŒ–
        import logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.historical_data_dir = "data/historical"
        self.cache = {}
        
        # æ£€æŸ¥æ•°æ®ç›®å½•
        if not os.path.exists(self.historical_data_dir):
            self.logger.warning(f"å†å²æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.historical_data_dir}")
            os.makedirs(self.historical_data_dir, exist_ok=True)
    
    def get_historical_data(self, symbol: str, timeframe: str = '1d', 
                          start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """
        è·å–å†å²æ•°æ® - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å®Œæ•´æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ (BTC/USDT, ETH/USDTç­‰)
            timeframe: æ—¶é—´æ¡†æ¶ (1d, 4h, 1hç­‰)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        """
        try:
            # è½¬æ¢ç¬¦å·æ ¼å¼
            binance_symbol = self._convert_to_binance_symbol(symbol)
            
            # æ„å»ºæ–‡ä»¶å
            filename = f"{binance_symbol}_{timeframe}_binance.parquet"
            file_path = os.path.join(self.historical_data_dir, filename)
            
            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
            if os.path.exists(file_path):
                self.logger.info(f"ä½¿ç”¨æœ¬åœ°æ•°æ®: {filename}")
                data = pd.read_parquet(file_path)
                
                # ç¡®ä¿datetimeåˆ—æ˜¯datetimeç±»å‹
                if 'datetime' in data.columns:
                    data['datetime'] = pd.to_datetime(data['datetime'])
                
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                if start_date:
                    data = data[data['datetime'] >= pd.to_datetime(start_date)]
                if end_date:
                    data = data[data['datetime'] <= pd.to_datetime(end_date)]
                
                self.logger.info(f"âœ… æœ¬åœ°æ•°æ®åŠ è½½æˆåŠŸ: {len(data)} æ¡è®°å½•")
                self.logger.info(f"   æ—¶é—´èŒƒå›´: {data['datetime'].min()} åˆ° {data['datetime'].max()}")
                
                return data
            
            # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•CSVæ ¼å¼
            csv_filename = f"{binance_symbol}_{timeframe}_binance.csv"
            csv_file_path = os.path.join(self.historical_data_dir, csv_filename)
            
            if os.path.exists(csv_file_path):
                self.logger.info(f"ä½¿ç”¨æœ¬åœ°CSVæ•°æ®: {csv_filename}")
                data = pd.read_csv(csv_file_path)
                data['datetime'] = pd.to_datetime(data['datetime'])
                
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                if start_date:
                    data = data[data['datetime'] >= pd.to_datetime(start_date)]
                if end_date:
                    data = data[data['datetime'] <= pd.to_datetime(end_date)]
                
                self.logger.info(f"âœ… æœ¬åœ°CSVæ•°æ®åŠ è½½æˆåŠŸ: {len(data)} æ¡è®°å½•")
                return data
            
            # å¦‚æœæœ¬åœ°æ•°æ®ä¸å­˜åœ¨ï¼Œç”Ÿæˆè­¦å‘Šå¹¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            self.logger.warning(f"æœ¬åœ°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            self.logger.warning("è¯·è¿è¡Œ python3 scripts/fix_data_source.py è·å–å®Œæ•´æ•°æ®")
            
            return self._generate_fallback_data(symbol, timeframe, start_date, end_date)
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥ {symbol}: {str(e)}")
            return self._generate_fallback_data(symbol, timeframe, start_date, end_date)
    
    def _convert_to_binance_symbol(self, symbol: str) -> str:
        """è½¬æ¢ä¸ºBinanceç¬¦å·æ ¼å¼"""
        # ç§»é™¤åˆ†éš”ç¬¦
        return symbol.replace('/', '').replace('-', '')
    
    def _generate_fallback_data(self, symbol: str, timeframe: str = '1d',
                               start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """ç”Ÿæˆå¤‡ç”¨æ¨¡æ‹Ÿæ•°æ®"""
        self.logger.warning(f"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {symbol}")
        
        # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
        if not start_date:
            start_date = '2024-01-01'
        if not end_date:
            end_date = '2024-12-31'
        
        # æ ¹æ®æ—¶é—´æ¡†æ¶ç”Ÿæˆæ—¥æœŸèŒƒå›´
        freq_map = {
            '1m': '1min', '5m': '5min', '15m': '15min', '30m': '30min',
            '1h': '1H', '4h': '4H', '1d': '1D'
        }
        
        freq = freq_map.get(timeframe, '1D')
        dates = pd.date_range(start=start_date, end=end_date, freq=freq)
        
        # æ¨¡æ‹Ÿä»·æ ¼æ•°æ®
        np.random.seed(42)
        base_price = 50000 if 'BTC' in symbol else 3000 if 'ETH' in symbol else 500
        
        prices = []
        current_price = base_price
        
        for _ in range(len(dates)):
            # éšæœºæ¸¸èµ°
            change = np.random.normal(0, 0.02)  # 2%æ ‡å‡†å·®
            current_price *= (1 + change)
            prices.append(current_price)
        
        # ç”ŸæˆOHLCVæ•°æ®
        data = []
        for i, (date, close) in enumerate(zip(dates, prices)):
            high = close * (1 + abs(np.random.normal(0, 0.01)))
            low = close * (1 - abs(np.random.normal(0, 0.01)))
            open_price = prices[i-1] if i > 0 else close
            volume = np.random.randint(1000000, 10000000)
            
            data.append({
                'datetime': date,
                'open': open_price,
                'high': max(open_price, high, close),
                'low': min(open_price, low, close),
                'close': close,
                'volume': volume
            })
        
        df = pd.DataFrame(data)
        self.logger.info(f"ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {len(df)} æ¡è®°å½•")
        
        return df
    
    def get_available_data_files(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
        if not os.path.exists(self.historical_data_dir):
            return []
        
        files = []
        for ext in ['*.parquet', '*.csv']:
            pattern = os.path.join(self.historical_data_dir, ext)
            files.extend(glob.glob(pattern))
        
        return [os.path.basename(f) for f in files]
    
    def get_data_summary(self) -> Dict:
        """è·å–æ•°æ®æ‘˜è¦ä¿¡æ¯"""
        summary = {
            'total_files': 0,
            'symbols': set(),
            'timeframes': set(),
            'date_ranges': {}
        }
        
        files = self.get_available_data_files()
        summary['total_files'] = len(files)
        
        for file in files:
            if '_binance.' in file:
                parts = file.split('_')
                if len(parts) >= 3:
                    symbol = parts[0]
                    timeframe = parts[1]
                    
                    summary['symbols'].add(symbol)
                    summary['timeframes'].add(timeframe)
                    
                    # å°è¯•è¯»å–æ–‡ä»¶è·å–æ—¥æœŸèŒƒå›´
                    try:
                        file_path = os.path.join(self.historical_data_dir, file)
                        if file.endswith('.parquet'):
                            df = pd.read_parquet(file_path)
                        else:
                            df = pd.read_csv(file_path)
                        
                        df['datetime'] = pd.to_datetime(df['datetime'])
                        date_range = f"{df['datetime'].min()} åˆ° {df['datetime'].max()}"
                        summary['date_ranges'][f"{symbol}_{timeframe}"] = {
                            'range': date_range,
                            'count': len(df)
                        }
                    except:
                        pass
        
        summary['symbols'] = list(summary['symbols'])
        summary['timeframes'] = list(summary['timeframes'])
        
        return summary
    
    def validate_data_quality(self, symbol: str, timeframe: str = '1d') -> Dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        try:
            data = self.get_historical_data(symbol, timeframe)
            
            if data.empty:
                return {'status': 'error', 'message': 'æ•°æ®ä¸ºç©º'}
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            required_columns = ['datetime', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in data.columns]
            
            if missing_columns:
                return {
                    'status': 'error', 
                    'message': f'ç¼ºå°‘åˆ—: {missing_columns}'
                }
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            quality_issues = []
            
            # æ£€æŸ¥ç©ºå€¼
            null_counts = data.isnull().sum()
            if null_counts.sum() > 0:
                quality_issues.append(f"ç©ºå€¼: {null_counts.to_dict()}")
            
            # æ£€æŸ¥ä»·æ ¼é€»è¾‘
            invalid_prices = data[(data['high'] < data['low']) | 
                                (data['high'] < data['open']) | 
                                (data['high'] < data['close']) |
                                (data['low'] > data['open']) | 
                                (data['low'] > data['close'])]
            
            if len(invalid_prices) > 0:
                quality_issues.append(f"ä»·æ ¼é€»è¾‘é”™è¯¯: {len(invalid_prices)} æ¡")
            
            # æ£€æŸ¥é‡å¤æ—¶é—´
            duplicate_times = data['datetime'].duplicated().sum()
            if duplicate_times > 0:
                quality_issues.append(f"é‡å¤æ—¶é—´: {duplicate_times} æ¡")
            
            return {
                'status': 'success',
                'total_records': len(data),
                'date_range': f"{data['datetime'].min()} åˆ° {data['datetime'].max()}",
                'quality_issues': quality_issues if quality_issues else ['æ— é—®é¢˜'],
                'price_range': f"${data['close'].min():.2f} - ${data['close'].max():.2f}"
            }
            
        except Exception as e:
            return {'status': 'error', 'message': str(e)}


def test_enhanced_data_module():
    """æµ‹è¯•å¢å¼ºæ•°æ®æ¨¡å—"""
    print("ğŸ” æµ‹è¯•å¢å¼ºæ•°æ®æ¨¡å—...")
    
    data_module = EnhancedDataModule()
    
    # 1. æ£€æŸ¥å¯ç”¨æ•°æ®æ–‡ä»¶
    print("\nğŸ“ å¯ç”¨æ•°æ®æ–‡ä»¶:")
    files = data_module.get_available_data_files()
    for file in files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"   ğŸ“„ {file}")
    
    if len(files) > 10:
        print(f"   ... è¿˜æœ‰ {len(files) - 10} ä¸ªæ–‡ä»¶")
    
    # 2. è·å–æ•°æ®æ‘˜è¦
    print("\nğŸ“Š æ•°æ®æ‘˜è¦:")
    summary = data_module.get_data_summary()
    print(f"   æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"   å¯ç”¨å¸ç§: {summary['symbols']}")
    print(f"   æ—¶é—´æ¡†æ¶: {summary['timeframes']}")
    
    # 3. æµ‹è¯•æ•°æ®åŠ è½½
    print("\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½:")
    test_cases = [
        ('BTC/USDT', '1d'),
        ('ETH/USDT', '4h'),
        ('BNB/USDT', '1h')
    ]
    
    for symbol, timeframe in test_cases:
        print(f"\n   æµ‹è¯• {symbol} {timeframe}:")
        
        # åŠ è½½æ•°æ®
        data = data_module.get_historical_data(symbol, timeframe)
        print(f"   âœ… æ•°æ®æ¡æ•°: {len(data)}")
        
        if len(data) > 0:
            print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {data['datetime'].min()} åˆ° {data['datetime'].max()}")
            print(f"   ğŸ’° ä»·æ ¼èŒƒå›´: ${data['close'].min():.2f} - ${data['close'].max():.2f}")
        
        # éªŒè¯æ•°æ®è´¨é‡
        quality = data_module.validate_data_quality(symbol, timeframe)
        print(f"   ğŸ” æ•°æ®è´¨é‡: {quality['status']}")
        if quality['status'] == 'success':
            print(f"   ğŸ“Š è´¨é‡é—®é¢˜: {quality['quality_issues']}")
    
    print("\nğŸ‰ å¢å¼ºæ•°æ®æ¨¡å—æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_enhanced_data_module()
